import numpy as np
import torch
import torch.nn.functional as functional

from typing import Dict, Any

from mass.nn.projection_layer import ProjectionLayer
from mass.utils.projection import project_camera_rays
from mass.utils.projection import transform_rays
from mass.utils.projection import bin_rays
from mass.utils.projection import update_feature_map
from mass.utils.projection import spherical_to_cartesian


class BaseProjectionLayer(torch.nn.Module, ProjectionLayer):
    """Create a feature projection layer in PyTorch that maintains a voxel
    grid description of the world world, where each voxel grid cell has
    a feature vector associated with it, typically semantic labels.

    Arguments:

    camera_height: int
        the map_height of the image generated by a pinhole camera onboard the
        agent, corresponding to a map_depth and semantic observation.
    camera_width: int
        the map_width of the image generated by a pinhole camera onboard the
        agent, corresponding to a map_depth and semantic observation.
    vertical_fov: float
        the vertical field of view of the onboard camera, measure in
        radians from the bottom of the viewport to the top.

    map_height: int
        the number of grid cells along the 'map_height' axis of the semantic
        map, as rendered using the top down rendering function.
    map_width: int
        the number of grid cells along the 'map_width' axis of the semantic
        map, as rendered using the top down rendering function.
    map_depth: int
        the number of grid cells that are collapsed along the 'up'
        direction by the top down rendering function.
    feature_size: int
        the number of units in each feature vector associated with every
        grid cell, such as the number of image segmentation categories.

    origin_y: float
        the center of the semantic map along the 'map_height' axis of the
        semantic map as viewed from a top-down render of the map.
    origin_x: float
        the center of the semantic map along the 'map_width' axis of the
        semantic map as viewed from a top-down render of the map.
    origin_z: float
        the center of the semantic map along the 'map_depth' axis of the
        semantic map as viewed from a top-down render of the map.
    grid_resolution: float
        the length of a single side of each voxel in the semantic map in
        units of the world coordinate system, which is typically meters.

    interpolation_weight: float
        float representing the interpolation weight used when adding
        new features in the feature map weighted by interpolation_weight.
    initial_feature_map: Optional[torch.Tensor]
        tensor representing the initial feature map tensor,
        which will be set to zero if the value not specified by default.

    """

    def __init__(self, camera_height: int = 224, camera_width: int = 224,
                 vertical_fov: float = 90.0, map_height: int = 256,
                 map_width: int = 256, map_depth: int = 64,
                 feature_size: int = 1, dtype: torch.dtype = torch.float32,
                 origin_y: float = 0.0, origin_x: float = 0.0,
                 origin_z: float = 0.0, grid_resolution: float = 0.05,
                 interpolation_weight: float = 0.5,
                 initial_feature_map: torch.Tensor = None):
        """Create a feature projection layer in PyTorch that maintains a voxel
        grid description of the world world, where each voxel grid cell has
        a feature vector associated with it, typically semantic labels.

        Arguments:

        camera_height: int
            the map_height of the image generated by a pinhole camera onboard the
            agent, corresponding to a map_depth and semantic observation.
        camera_width: int
            the map_width of the image generated by a pinhole camera onboard the
            agent, corresponding to a map_depth and semantic observation.
        vertical_fov: float
            the vertical field of view of the onboard camera, measure in
            radians from the bottom of the viewport to the top.

        map_height: int
            the number of grid cells along the 'map_height' axis of the semantic
            map, as rendered using the top down rendering function.
        map_width: int
            the number of grid cells along the 'map_width' axis of the semantic
            map, as rendered using the top down rendering function.
        map_depth: int
            the number of grid cells that are collapsed along the 'up'
            direction by the top down rendering function.
        feature_size: int
            the number of units in each feature vector associated with every
            grid cell, such as the number of image segmentation categories.

        origin_y: float
            the center of the semantic map along the 'map_height' axis of the
            semantic map as viewed from a top-down render of the map.
        origin_x: float
            the center of the semantic map along the 'map_width' axis of the
            semantic map as viewed from a top-down render of the map.
        origin_z: float
            the center of the semantic map along the 'map_depth' axis of the
            semantic map as viewed from a top-down render of the map.
        grid_resolution: float
            the length of a single side of each voxel in the semantic map in
            units of the world coordinate system, which is typically meters.

        interpolation_weight: float
            float representing the interpolation weight used when adding
            new features in the feature map weighted by interpolation_weight.
        initial_feature_map: Optional[torch.Tensor]
            tensor representing the initial feature map tensor,
            which will be set to zero if the value not specified by default.

        """

        super(BaseProjectionLayer, self).__init__()

        # arguments that control the calibration of the camera where
        # camera_height and vertical_fov are used to compute focal length
        self.interpolation_weight = interpolation_weight
        self.camera_height = camera_height
        self.camera_width = camera_width
        self.vertical_fov = vertical_fov

        # arguments that specify the number of voxels in the map and
        # the number of features stored in each voxel
        self.map_height = map_height
        self.map_width = map_width
        self.map_depth = map_depth
        self.feature_size = feature_size

        # arguments that specify how to convert between coordinate systems
        # the origin corresponds to the center of the map tensor
        self.origin_x = origin_x
        self.origin_y = origin_y
        self.origin_z = origin_z
        self.grid_resolution = grid_resolution  # size of voxels in meters

        # pre-generate a set of rays emanating from a pinhole camera where
        # the number of rays may be an integer multiple of the obs resolution
        focal_length = (camera_height / 2.0 /
                        np.tan(np.radians(vertical_fov) / 2.0))
        self.register_buffer('rays', project_camera_rays(
            camera_height, camera_width, focal_length, focal_length))

        # create an initially empty map tensor by filling it with zeros
        # which will store features projected from observations to the world
        self.register_buffer('data', torch.zeros(
            map_height, map_width, map_depth, feature_size, dtype=dtype)
            if initial_feature_map is None else initial_feature_map)

        # bins for translating from world coordinates to the map
        # by binning points along the 'map_width' axis
        max_x = origin_x + (map_width + 1) * grid_resolution / 2 - 1e-6
        min_x = origin_x - (map_width + 1) * grid_resolution / 2
        self.register_buffer('bins_x', torch.arange(
            min_x, max_x, grid_resolution, dtype=torch.float32))

        # bins for translating from world coordinates to the map
        # by binning points along the 'map_height' axis
        max_y = origin_y + (map_height + 1) * grid_resolution / 2 - 1e-6
        min_y = origin_y - (map_height + 1) * grid_resolution / 2
        self.register_buffer('bins_y', torch.arange(
            min_y, max_y, grid_resolution, dtype=torch.float32))

        # bins for translating from world coordinates to the map
        # by binning points along the 'map_depth' axis
        max_z = origin_z + (map_depth + 1) * grid_resolution / 2 - 1e-6
        min_z = origin_z - (map_depth + 1) * grid_resolution / 2
        self.register_buffer('bins_z', torch.arange(
            min_z, max_z, grid_resolution, dtype=torch.float32))

    def reset(self, origin_y: float = 0.0,
              origin_x: float = 0.0, origin_z: float = 0.0):
        """Utility function for clearing the contents of the feature map,
        which is typically called at the beginning of an episode with
        a new map origin, in order to reduce gpu memory usage.

        Arguments:

        origin_y: float
            the center of the semantic map along the 'map_height' axis of the
            semantic map as viewed from a top-down render of the map.
        origin_x: float
            the center of the semantic map along the 'map_width' axis of the
            semantic map as viewed from a top-down render of the map.
        origin_z: float
            the center of the semantic map along the 'map_depth' axis of the
            semantic map as viewed from a top-down render of the map.

        """

        # arguments that specify how to convert between coordinate systems
        # the origin corresponds to the center of the map tensor
        self.origin_x = origin_x
        self.origin_y = origin_y
        self.origin_z = origin_z
        self.data.zero_()  # set the feature map to zero

        # bins for translating from world coordinates to the map
        # by binning points along the 'map_width' axis
        max_x = origin_x + (self.map_width +
                            1) * self.grid_resolution / 2 - 1e-6
        min_x = origin_x - (self.map_width +
                            1) * self.grid_resolution / 2
        self.bins_x.copy_(torch.arange(
            min_x, max_x, self.grid_resolution, dtype=torch.float32))

        # bins for translating from world coordinates to the map
        # by binning points along the 'map_height' axis
        max_y = origin_y + (self.map_height +
                            1) * self.grid_resolution / 2 - 1e-6
        min_y = origin_y - (self.map_height +
                            1) * self.grid_resolution / 2
        self.bins_y.copy_(torch.arange(
            min_y, max_y, self.grid_resolution, dtype=torch.float32))

        # bins for translating from world coordinates to the map
        # by binning points along the 'map_depth' axis
        max_z = origin_z + (self.map_depth +
                            1) * self.grid_resolution / 2 - 1e-6
        min_z = origin_z - (self.map_depth +
                            1) * self.grid_resolution / 2
        self.bins_z.copy_(torch.arange(
            min_z, max_z, self.grid_resolution, dtype=torch.float32))

    def get_feature_map(self):
        """Helper function that returns the wrapped feature map tensor that
        observed features are projected onto during training, that may
        require optional arguments to discern between maps.

        Returns:

        feature_map: torch.Tensor
            tensor representing a three dimensional grid of voxels with
            features projected onto it from incoming observations.

        """

        return self.data  # users should not access data directly

    def forward(self, observation: Dict[str, torch.Tensor]):
        """Update the semantic map given a map_depth image and a feature image
        by projecting the features onto voxels in the semantic map using
        a set of rays emanating from a virtual pinhole camera.

        Arguments:

        observation["position"]: torch.Tensor
            the position of the agent in the world coordinate system, where
            the position will be binned to voxels in a semantic map.
        observation["yaw"]: torch.Tensor
            a tensor representing the yaw in radians of the coordinate,
            starting from the x-axis and turning counter-clockwise.
        observation["elevation"]: torch.Tensor
            a tensor representing the elevation in radians about the x-axis,
            with positive corresponding to upwards tilt.

        observation["map_depth"]: torch.FloatTensor
            the length of the corresponding ray in world coordinates before
            hitting a surface, with shape: [height, width, 1].
        observation["features"]: Any
            a feature vector for every pixel in the imaging plane, to be
            scattered on the map, with shape: [height, width, num_classes].

        """

        # call update and return the feature map which can
        self.update(observation)  # be passed to subsequent layers
        return self.get_feature_map()  # such as for learning semantic search

    def update(self, observation: Dict[str, torch.Tensor]):
        """Update the semantic map given a map_depth image and a feature image
        by projecting the features onto voxels in the semantic map using
        a set of rays emanating from a virtual pinhole camera.

        Arguments:

        observation["position"]: torch.Tensor
            the position of the agent in the world coordinate system, where
            the position will be binned to voxels in a semantic map.
        observation["yaw"]: torch.Tensor
            a tensor representing the yaw in radians of the coordinate,
            starting from the x-axis and turning counter-clockwise.
        observation["elevation"]: torch.Tensor
            a tensor representing the elevation in radians about the x-axis,
            with positive corresponding to upwards tilt.

        observation["map_depth"]: torch.FloatTensor
            the length of the corresponding ray in world coordinates before
            hitting a surface, with shape: [height, width, 1].
        observation["features"]: Any
            a feature vector for every pixel in the imaging plane, to be
            scattered on the map, with shape: [height, width, num_classes].

        """

        # ensure all tensors have the appropriate device and dtype
        position = torch.as_tensor(observation[
            "position"], dtype=torch.float32, device=self.data.device)
        yaw = torch.as_tensor(observation[
            "yaw"], dtype=torch.float32, device=self.data.device)
        elevation = torch.as_tensor(observation[
            "elevation"], dtype=torch.float32, device=self.data.device)
        depth = torch.as_tensor(observation[
            "depth"], dtype=torch.float32, device=self.data.device)

        # the features should have the same dtype as the map
        # and should be expanded to the same shape as camera rays
        features = torch.as_tensor(observation[
            "features"], dtype=self.data.dtype, device=self.data.device)
        features = torch.repeat_interleave(
            features, self.camera_height // features.shape[0], dim=0)
        features = torch.repeat_interleave(
            features, self.camera_width // features.shape[1], dim=1)

        # orient rays in the direction of the agent's onboard camera
        oriented_rays = transform_rays(
            self.rays,
            spherical_to_cartesian(yaw, elevation),
            spherical_to_cartesian(yaw, elevation + np.pi / 2))

        # determine which voxels ray endpoints fall into on the map
        ind_x, ind_y, ind_z, ratio_x, ratio_y, ratio_z, features = bin_rays(
            self.bins_x, self.bins_y, self.bins_z,
            position, oriented_rays, depth, features)

        # scatter features from the imaging plane onto the semantic map
        update_feature_map(ind_y, ind_x, ind_z, ratio_y, ratio_x, ratio_z,
                           features, self.data, interpolation_weight=
                           self.interpolation_weight)

        return self  # return self for chaining functions

    def top_down(self, depth_slice: slice = slice(0, 32)):
        """Render a top-down view of a map of features organized as a three
        dimensional grid of voxels, by taking the zero vector to be empty
        voxels and rendering the top-most non-empty voxel to a pixel in a grid.

        Arguments:

        depth_slice: slice
            an slice that specifies which map_depth components to use
            when rendering a top down visualization of the feature map.

        Returns:

        feature_image: torch.Tensor
            an image with a feature vector associated with every cell in the
            image corresponding to a visible voxel in the original feature map.

        """

        # create a mask for which voxels are occupied by checking equality with
        # zero and aggregating over the features axis
        feature_map = (self.data[:, :, depth_slice]
                       if depth_slice is not None else self.data)
        mask = torch.ne(feature_map, 0).any(
            dim=-1, keepdim=True).to(dtype=feature_map.dtype)

        # determine the index of the voxel in each spatial location that is
        # visible by taking maximizing map_height over visible voxels
        idx = (mask.cumsum(dim=-2) * mask).argmax(dim=-2, keepdim=True)

        # use gather rather than a sum to aggregate features and pick out the
        # feature vector that is actually rendered
        shape = list(feature_map.shape[:-2]) + [1, feature_map.shape[-1]]
        return torch.gather(feature_map, -2,
                            idx.expand(*shape)).squeeze(-2)

    def clamp_to_world(self, coords):
        """Utility function that clamps the coordinates specified to the
        range supported by the semantic map, which is calculated as between
        the midpoints of voxel extrema along each of the coordinate axes.

        Arguments:

        coords: torch.Tensor
            a set of coordinates in xyz order in the coordinate system of the
            world, to be clamped to the range supported by the map.

        Returns:

        coords: torch.Tensor
            a set of coordinates in xyz order in the coordinate system of the
            world, to be clamped to the range supported by the map.

        """

        # ensure that all tensors are on the appropriate compute device
        kwargs = dict(dtype=torch.float32, device=self.data.device)
        coords = torch.as_tensor(coords, **kwargs)

        # clamp the xyz coordinates to the midpoints of the bins used to
        # generate voxels, and ensure that no coordinates are out of bounds
        upper = torch.stack([(self.bins_x[-1] + self.bins_x[-2]) / 2,
                             (self.bins_y[-1] + self.bins_y[-2]) / 2,
                             (self.bins_z[-1] + self.bins_z[-2]) / 2])
        lower = torch.stack([(self.bins_x[0] + self.bins_x[1]) / 2,
                             (self.bins_y[0] + self.bins_y[1]) / 2,
                             (self.bins_z[0] + self.bins_z[1]) / 2])

        # clamp the coordinates to the midpoints of the voxel extrema
        shape = [1 for _ in coords.shape[:-1]] + [3]
        return coords.clamp(min=lower.view(*shape)[..., :coords.shape[-1]],
                            max=upper.view(*shape)[..., :coords.shape[-1]])

    def clamp_to_map(self, coords):
        """Utility function that clamps the coordinates specified to the
        range supported by the semantic map, which is calculated as between
        the midpoints of voxel extrema along each of the coordinate axes.

        Arguments:

        coords: torch.Tensor
            a set of coordinates in xyz order in the coordinate system of the
            map, to be clamped to the range supported by the map.

        Returns:

        coords: torch.Tensor
            a set of coordinates in xyz order in the coordinate system of the
            map, to be clamped to the range supported by the map.

        """

        # ensure that all tensors are on the appropriate compute device
        kwargs = dict(dtype=coords.dtype, device=self.data.device)
        coords = torch.as_tensor(coords, **kwargs)

        # clamp the xyz coordinates to the midpoints of the bins used to
        # generate voxels, and ensure that no coordinates are out of bounds
        lower = torch.tensor([0, 0, 0], **kwargs)
        upper = torch.tensor([self.map_width - 1, self.map_height - 1,
                              self.map_depth - 1], **kwargs)

        # clamp the coordinates to the midpoints of the voxel extrema
        shape = [1 for _ in coords.shape[:-1]] + [3]
        return coords.clamp(min=lower[:coords.shape[-1]].view(*shape),
                            max=upper[:coords.shape[-1]].view(*shape))

    def map_to_world(self, coords):
        """Transform coordinates from the map coordinate system to the world
        coordinate system, assuming a, xyz convention even in the map
        coordinate system, even though it is stored as yxz.

        Arguments:

        coords: torch.Tensor
            a set of coordinates in xyz order in the coordinate system of the
            map to be converted to the world coordinate system.

        Returns:

        world_coords: torch.Tensor
            a set of coordinates in xyz order in the coordinate system of the
            world that have been converted from the map system.

        """

        # obtain the indices of the bins that map coordinates fall into
        # and collect their world coordinates into a list
        coords = self.clamp_to_map(coords).to(dtype=torch.float32)
        floored_coords = coords.floor()
        bins_l, bins_r = [], []
        idx = floored_coords.to(dtype=torch.int64)

        # compute the midpoints of voxel bins along the x and y axes
        bins_x = (self.bins_x[:-1] +  # used to lookup world coordinates
                  self.bins_x[1:]).view(-1, 1) / 2  # from map coordinates
        bins_y = (self.bins_y[:-1] +
                  self.bins_y[1:]).flip(-1).view(-1, 1) / 2

        # lookup coordinates of left / right bin boundaries along x axis
        bins_l.append(functional.embedding(idx[..., 0], bins_x))
        bins_r.append(functional.embedding(
            (idx[..., 0] + 1).clamp(
                min=0, max=self.map_width - 1), bins_x))

        # lookup coordinates of left / right bin boundaries along y axis
        bins_l.append(functional.embedding(idx[..., 1], bins_y))
        bins_r.append(functional.embedding(
            (idx[..., 1] + 1).clamp(
                min=0, max=self.map_height - 1), bins_y))

        if coords.shape[-1] == 3:  # also support xy vectors

            bins_z = (self.bins_z[:-1] +
                      self.bins_z[1:]).view(-1, 1) / 2

            # coordinates of left / right bin boundaries along z axis
            bins_l.append(functional.embedding(idx[..., 2], bins_z))
            bins_r.append(functional.embedding(
                (idx[..., 2] + 1).clamp(
                    min=0, max=self.map_depth - 1), bins_z))

        # interpolate between the left and right bin boundaries based
        # on how far between voxels map coordinates fall
        bins_l = torch.cat(bins_l, dim=-1)
        bins_r = torch.cat(bins_r, dim=-1)
        return bins_l + (bins_r - bins_l) * (coords - floored_coords)

    def world_to_map(self, coords):
        """Transform coordinates from the world coordinate system to the map
        coordinate system, assuming a, xyz convention even in the map
        coordinate system, even though it is stored as yxz.

        Arguments:

        world_coords: torch.Tensor
            a set of coordinates in xyz order in the coordinate system of the
            world that will be converted to the map system.

        Returns:

        coords: torch.Tensor
            a set of coordinates in xyz order in the coordinate system of the
            map that were converted from the world coordinate system.

        """

        # ensure coordinates reside within the bounds of the semantic map
        coords = self.clamp_to_world(coords)

        # lookup which bins the coordinates fall into, noting that the
        # index of the first bin is one rather than zero
        bins = [torch.bucketize(coords[..., 0].contiguous(),
                                self.bins_x, right=True) - 1,
                self.bins_y.size(dim=0) -  # flip the index by convention
                torch.bucketize(coords[..., 1].contiguous(),
                                self.bins_y, right=True) - 1]

        if coords.shape[-1] == 3:  # also support xy vectors
            bins.append(torch.bucketize(coords[..., 2].contiguous(),
                                        self.bins_z, right=True) - 1)

        return torch.stack(bins, dim=-1)  # return bins in xyz order

    def visualize(self, obs: Dict[str, Any],
                  depth_slice: slice = slice(0, 32)):
        """Helper function that returns an image that is used for
        visualizing contents of the feature map contained in subclasses,
        such as visualizing object categories, or which voxels are obstacles.

        Arguments:

        obs: Dict[str, Any]
            the current observation, as a dict or Tensor, which can be
            used to visualize the current location of the agent in the scene.
        slice: slice
            an slice that specifies which map_depth components to use
            when rendering a top down visualization of the feature map.

        Returns:

        image: np.ndarray
            numpy array representing a visualization the contents of this
            layer, such as an image showing object categories.

        """

        # an image where ones indicate free space and zeros indicate
        # a voxel in that spatial location is blocked
        return 1.0 - np.tile(torch.ne(
            self.data[:, :, depth_slice]
            if depth_slice is not None else
            self.data, 0).any(dim=-1, keepdim=True).to(
            dtype=torch.float32).detach().cpu().numpy(), (1, 1, 3))
